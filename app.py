import os
import streamlit as st
import google.generativeai as genai

# --- ãƒšãƒ¼ã‚¸è¨­å®š ---
st.set_page_config(
    page_title="Tag-VI Agent",
    page_icon="ğŸ§ ",
    layout="wide"
)

# --- ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ ---
st.title("ğŸ§  æ§‹é€ åŒ–æ€è€ƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ Tag-VI")
st.markdown("""
ã‚ãªãŸã®æ‚©ã¿ã‚„èª²é¡Œã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
ç‹¬è‡ªãƒ—ãƒ­ãƒˆã‚³ãƒ«**ã€ŒèªçŸ¥åˆ†é¡æ³•ã‚¿ã‚°6å±¤ã€**ã«åŸºã¥ãã€æ§‹é€ çš„ãªåˆ†æã¨æœ¬è³ªçš„ãªè§£æ±ºç­–ã‚’æç¤ºã—ã¾ã™ã€‚
""")

# --- APIã‚­ãƒ¼ã®å–å¾— ---
# Cloud Runãªã©ã®ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€å®‰å…¨ãªè¨­è¨ˆ
api_key = os.environ.get("GOOGLE_API_KEY")

if not api_key:
    st.error("âš ï¸ APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- Geminiã®è¨­å®š ---
genai.configure(api_key=api_key)

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆã‚¿ã‚°6å±¤ã®å®šç¾©ï¼‰
SYSTEM_PROMPT = """
{
  "protocol_name": "Cognitive_Tagging_6Layers",
  "description": "Information structuring protocol to minimize semantic drift and hallucinations.",
  "layers": {
    "L1_Surface": {
      "desc": "Category or Topic",
      "example": ["AI", "Economics"]
    },
    "L2_Structure": {
      "desc": "Mechanism, Causality, Pattern",
      "example": ["Integration", "Feedback Loop"]
    },
    "L3_Context": {
      "desc": "Time, Culture, History",
      "example": ["2025s", "Post-Modern"]
    },
    "L4_Philosophy": {
      "desc": "Values, Beliefs, Ethics",
      "example": ["Rationality", "Open Source Spirit"]
    },
    "L5_Cognition": {
      "desc": "Sensation, Aesthetic, Vibe",
      "example": ["Immersive", "Minimalist"]
    },
    "L6_Meta": {
      "desc": "Operational Rules",
      "example": ["Use Python", "Output as JSON"]
    }
  },
  "instruction": "Analyze the user input based on these 6 layers before generating a response. Output must include specific analysis for each layer (L1-L6) and a final structural conclusion."
}
"""

# ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
model = genai.GenerativeModel(
    model_name="gemini-1.5-flash", # é«˜é€Ÿã§å®‰å®šã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®š
    system_instruction=SYSTEM_PROMPT
)

# --- ãƒãƒ£ãƒƒãƒˆç”»é¢ã®æ§‹ç¯‰ ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®å±¥æ­´ã‚’è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("ä¾‹ï¼šç¾å ´ã®è·äººãŒãªã‹ãªã‹æ–°ã—ã„ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ãã‚Œãªã„â€¦"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’è¡¨ç¤º
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # AIã®å¿œç­”ç”Ÿæˆ
    with st.chat_message("assistant"):
        with st.spinner("ã‚¿ã‚°6å±¤ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã§æ§‹é€ è§£æä¸­..."):
            try:
                response = model.generate_content(prompt)
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            except Exception as e:
                st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
